{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15a88171",
   "metadata": {},
   "source": [
    "# Scraping de Metacritic\n",
    " L'objectif du scraping du site Metacritic est d'obtenir les informations ci-dessous :\n",
    " - Le nom du jeu-vidéo\n",
    " - la plateforme correspondant au jeu (sachant qu'un jeu peut être disponible sur plusieurs plateformes)\n",
    " - Le studio qui l'a développé\n",
    " - La note obtenue par le jeu auprès des utilisateurs\n",
    " - La note donnée par le site Metacritic\n",
    "\n",
    "La première étape consiste à vérifier les repertoires qui sont autorisés au scraping en regardant le fichier : https://www.metacritic.com/robots.txt.\n",
    "Les jeux-videos sur le site Metacritic sont rangés dans des listes déroulantes par plateformes (ex: Ps5, Ps4...). \n",
    "Dans chaque plateforme, nous avons des pages numérotées (Ex: page 1 à page 15) sur lesquelles nous avons les liens des jeux. \n",
    "Ces liens conduisent à une page de détails pour chaque jeu sur laquelle les informations recherchées sont disponibles.\n",
    "\n",
    "Pour arriver à scraper toutes les données, nous allons procéder comme suit :\n",
    "\n",
    "1. Charger les plateformes via https://www.metacritic.com/game (finalement manuellement en raison des différentes disparités sur les règles de nomenclature)\n",
    "2. Récupérer tous les liens des jeux\n",
    "3. Scraper les liens des jeux afin d'obtenir les informations détaillés\n",
    "4. Consituter le dataset final\n",
    "\n",
    "\n",
    "Pour en savoir plus sur les notes du site (Metascrore) : https://www.metacritic.com/about-metascores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e58207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importer les librairies utilisées\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from urllib.request import ssl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b3a9fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nombre des jeux-vidéos scrapés\n",
    "count = 0\n",
    "\n",
    "#urls non scrapés\n",
    "url_plateforme_non_scrapes = []\n",
    "url_page_non_scrapes = []\n",
    "url_game_non_scrapes = []\n",
    "url_games = []\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context #utiliser une connexion ssl\n",
    "\n",
    "def scrape_url(url):\n",
    "    \"\"\"\n",
    "    Cette fonction a pour rôle de scraper la page fournie par url\n",
    "    Nous utilisons l'agent Mozilla pour éviter des pare-feux \n",
    "    applicatifs qui peuvent bloquer les requêtes provenant directement de python\n",
    "    \"\"\"\n",
    "    try:\n",
    "        page = requests.get(url,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    except Exception as e:\n",
    "        print(\"url : \" + str(url))\n",
    "        print(\"Error :\" + str(e))\n",
    "        soup = []\n",
    "    return soup\n",
    "\n",
    "def __getDetailsGame__(game_url):\n",
    "    \"\"\"\n",
    "    Récupérer les données détaillées pour chaque jeu\n",
    "    \n",
    "    ******\n",
    "    paramètres: \n",
    "    game_url : url pour accéder aux données détaillées du jeu\n",
    "    ******\n",
    "    \"\"\"\n",
    "    url_details_game = \"https://www.metacritic.com\"+str(game_url)\n",
    "    \n",
    "    #initialisation de soup de beautiful\n",
    "    soup = []\n",
    "    data = []\n",
    "    \n",
    "    #Scraping de la page transmise\n",
    "    soup = scrape_url(url_details_game)\n",
    "    \n",
    "    \n",
    "    if(len(soup)==0):\n",
    "        print(\"Url inaccessible : \" + str(url_details_game))\n",
    "    else:\n",
    "        metascore_html = soup.find(name = 'span', attrs = {'itemprop': 'ratingValue'})\n",
    "        userscore_html_tbd = soup.find(name='div' , attrs={'class':'metascore_w user large game tbd'})\n",
    "        userscore_html_mixed = soup.find(name='div' , attrs={'class':'metascore_w user large game mixed'})\n",
    "        userscore_html_positive = soup.find(name='div' , attrs={'class':'metascore_w user large game positive'})\n",
    "        userscore_html_negative = soup.find(name='div' , attrs={'class':'metascore_w user large game negative'})\n",
    "        studio_html = soup.find(name = 'a', attrs = {'class': 'button'})\n",
    "        date_html = soup.select('.release_data .data')\n",
    "        game_name_html = soup.find(name='h1')\n",
    "        plateforme_html = soup.select('.platform a')\n",
    "        \n",
    "        \n",
    "        jeu = None if game_name_html == None else game_name_html.text.strip()\n",
    "        metascore = None if metascore_html== None else metascore_html.text.strip()\n",
    "        userscore_tdb = None if userscore_html_tbd== None else userscore_html_tbd.text.strip()\n",
    "        userscore_mixed = None if userscore_html_mixed== None else userscore_html_mixed.text.strip()\n",
    "        userscore_positive = None if userscore_html_positive== None else userscore_html_positive.text.strip()\n",
    "        userscore_negative = None if userscore_html_negative== None else userscore_html_negative.text.strip()\n",
    "        studio = None if studio_html== None else studio_html.text\n",
    "        date = None if date_html == [] else date_html[0].text.strip()\n",
    "        plateforme = None if plateforme_html == [] else plateforme_html[0].text.strip()\n",
    "        \n",
    "        \n",
    "        data = {\n",
    "            'jeu':jeu,\n",
    "            'plateforme-url':plateforme,\n",
    "            'date':date,\n",
    "            'metascore': metascore,\n",
    "            'userscore_tbd':userscore_tdb,\n",
    "            'userscore_mixed':userscore_mixed,\n",
    "            'userscore_positive':userscore_positive,\n",
    "            'userscore_negative':userscore_negative,\n",
    "            'studio':studio\n",
    "        }\n",
    "        \n",
    "        return data\n",
    "    \n",
    "\n",
    "#Les plateformes\n",
    "plateformes = ['ps4','ps5','xbox-series-x','xboxone','switch','pc','ios','stadia','ps3','ps2',\n",
    "           'ps','xbox360','xbox','wii','ds','gamecube','n64','gba','psp','dreamcast','wii-u','3ds','vita']\n",
    "\n",
    "#Fonction pour ajouter les links des jeux dans le tableau d'url à partir de la liste des plateformes\n",
    "def ___GetGameLinkFromPlateformList(plateformes):\n",
    "    \"\"\"\n",
    "    Cette fonction permet de récupérer les liens des jeux pour chaque plateforme.\n",
    "    ***************\n",
    "    Paramètres :\n",
    "    - plateforme : liste des plateformes disponibles \n",
    "    ***************\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    # Log Heure de début\n",
    "    print(\"Début : \" + str(datetime.today()))\n",
    "\n",
    "    #On parcours les plateformes pour trouver\n",
    "    for p in plateformes:\n",
    "        \"\"\"\n",
    "        Url qui liste les jeux par plateforme. \n",
    "        éSur cette partie nous pouvons avoir plusieurs page. \n",
    "        Nous recupérons le nombre max de page et faisons une boucle jusqu'a atteindre ce nombre\n",
    "        \"\"\"\n",
    "        url_principal = \"https://www.metacritic.com/browse/games/release-date/available/\"+p+\"/date\"\n",
    "\n",
    "\n",
    "        #scraper l'url principale\n",
    "        soup = scrape_url(url_principal)\n",
    "\n",
    "\n",
    "        #Si l'url scrapé renvoie une erreur, nous ne déroulons pas le process normal sinon nous le déroulons \n",
    "        if(len(soup)!=0):\n",
    "            num_page = soup.select(\".page_num\")\n",
    "\n",
    "            #Les numéros de page \n",
    "            #Si le numéro de page est valide, il ya plusieurs pages de liens de jeuxSinon il n'y a qu'une seule page à scraper\n",
    "            if(num_page):\n",
    "\n",
    "                #Nous retenons le numéro maximum de page\n",
    "                val_range = int(num_page[-1].text)\n",
    "\n",
    "\n",
    "                #Nous parcourons les pages à scraper pour obtenir tous les liens hypertexte des jeux\n",
    "                for i in range(0,val_range):\n",
    "                    #Url pour chaque page de liste de jeux par plateforme\n",
    "                    url_page_game = \"https://www.metacritic.com/browse/games/release-date/available/\"+str(p)+\"/date?page=\"+str(i)\n",
    "                    \n",
    "                    print(\"url de page numerotée : \" + url_page_game)\n",
    "                    #scraper l'url paginé\n",
    "                    soup = scrape_url(url_page_game) \n",
    "\n",
    "                    #Si le scraping s'est bien passé\n",
    "                    if(len(soup)!=0):\n",
    "                        #Récupération du bloc contenant les liens hypertextes\n",
    "                        bloc_list_game = soup.find_all('table', {'class':\"clamp-list\"})\n",
    "\n",
    "                        #Parcourir les blocs de liens\n",
    "                        for b in bloc_list_game:\n",
    "                            #Récupérer les liens\n",
    "                            for a in b.find_all('a', href=True,class_=\"title\"): \n",
    "                                if a.text: \n",
    "                                    url_games.append(a['href'])\n",
    "                                    count+=1\n",
    "                                    print(\"Nombre de liens : \" + str(count))\n",
    "                    else:\n",
    "                        url_page_non_scrapes.append(url_page_game)\n",
    "\n",
    "            else:\n",
    "                #Il y a une seule page à scraper\n",
    "                url_page_game = \"https://www.metacritic.com/browse/games/release-date/available/\"+str(p)+\"/date\"\n",
    "\n",
    "                #scraper l'url principale\n",
    "                soup = scrape_url(url_page_game)\n",
    "                \n",
    "                print(\"url de page directe : \" + url_page_game)\n",
    "\n",
    "                #Si le scraping s'est bien passé\n",
    "                if(len(soup)!=0):\n",
    "                    #Récupération du bloc contenant les liens hypertextes\n",
    "                        bloc_list_game = soup.find_all('table', {'class':\"clamp-list\"})\n",
    "\n",
    "                        #Parcourir les blocs de liens\n",
    "                        for b in bloc_list_game:\n",
    "                            #Récupérer les liens\n",
    "                            for a in b.find_all('a', href=True,class_=\"title\"): \n",
    "                                if a.text: \n",
    "                                    url_games.append(a['href'])\n",
    "                                    count+=1\n",
    "                                    print(\"Nombre de liens : \" + str(count))\n",
    "\n",
    "                else:\n",
    "                    url_page_non_scrapes.append(url_page_game)\n",
    "\n",
    "        else:\n",
    "            print(\"URL principale - scraping non valide\")\n",
    "            url_plateforme_non_scrapes.append(url_principal)\n",
    "\n",
    "    # Log Heure de fin\n",
    "    print(\"Fin : \" + str(datetime.today()))\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Charger les liens des plateformes\n",
    "___GetGameLinkFromPlateformList(plateformes)\n",
    "print(\"Nombre de liens de jeux : \" + str(len(url_games)))\n",
    "\n",
    "# téléchargeer les liens dans un csv à preserver pour éviter de reprendre tout en cas de souci de connexion ou de plantage de jupyter notebook\n",
    "df_links = pd.DataFrame(url_games,columns=['link'])\n",
    "\n",
    "df_links.to_csv('../data/temp/metacritic/links.csv',sep=',')\n",
    "\n",
    "# Récupérer les liens\n",
    "df_links = pd.read_csv('../data/temp/metacritic/links.csv',sep=',',index_col='Unnamed: 0')\n",
    "\n",
    "\"\"\"\n",
    "Il y avait environ 117000 liens à scraper. Le notebook jupyter se plantait fréquemment. J'ai donc opté pour un découpage du \n",
    "dataframe des liens par lot de 5000. J'ai exécuté la portion de code suivante dans 24 notebook avec en entrée chaque lot de 5000\n",
    "liens de jeux.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10654783",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pour chaque lien dans notre liste url_game, ou notre dataFrame links, nous allons récupérer les infos détaillées\n",
    "Ce bloc est à exécuter pour chaque lot de links ou en un bloc.\n",
    "\n",
    "Exemple de découpage avant exécution\n",
    "df_links = df_links[df_links.index<5000]\n",
    "\n",
    "\"\"\"\n",
    "#Tous les jeux-vidéos scrapés\n",
    "allgames = []\n",
    "count_games_data = 0\n",
    "\n",
    "for l in df_links.link:\n",
    "    #Récupérer les détails des jeux\n",
    "    allgames.append(__getDetailsGame__(l))\n",
    "    count_games_data+=1\n",
    "    print(\"Nombre de data-jeu scrapés : \" + str(count_games_data))\n",
    "\n",
    "\"\"\"\n",
    "Génération du dataset\n",
    "\"\"\"\n",
    "\n",
    "# Quel est le nombre total de ligne générées\n",
    "print(\"Nombre total de lignes générées:\" + str(len(allgames)))\n",
    "\n",
    "#Supprimer les lignes \"None\"\n",
    "allgames_save = [a for a in allgames if a is not None ]\n",
    "\n",
    "#Création du dataset\n",
    "dataset = pd.DataFrame(allgames_save)\n",
    "\n",
    "# Remplacer les nan(None) par 0\n",
    "dataset[\n",
    "    ['metascore',\n",
    "     'userscore_tbd',\n",
    "     'userscore_mixed',\n",
    "     'userscore_positive',\n",
    "     'userscore_negative']\n",
    "]= dataset[['metascore','userscore_tbd','userscore_mixed','userscore_positive','userscore_negative']].fillna('0')\n",
    "\n",
    "# Remplacer les tdb par 0\n",
    "dataset['userscore_tbd'] = dataset['userscore_tbd'].replace('tbd','0')\n",
    "\n",
    "#Convertir les valeurs des notes en float\n",
    "dataset[\n",
    "    ['metascore',\n",
    "     'userscore_tbd',\n",
    "     'userscore_mixed',\n",
    "     'userscore_positive',\n",
    "     'userscore_negative']\n",
    "] = dataset[\n",
    "    ['metascore',\n",
    "     'userscore_tbd',\n",
    "     'userscore_mixed',\n",
    "     'userscore_positive',\n",
    "     'userscore_negative']\n",
    "].astype('float')\n",
    "\n",
    "#Trouver la note user en additionnant les différentes notes: l'une des notes seulement renseignée\n",
    "dataset['userscore'] = dataset['userscore_tbd'] + dataset['userscore_mixed'] + dataset['userscore_positive'] + dataset['userscore_negative']\n",
    "\n",
    "#Supprimer les notes intermédiaires\n",
    "dataset = dataset.drop(['userscore_tbd','userscore_mixed','userscore_positive','userscore_negative'],axis=1)\n",
    "dataset.to_csv('../data/temp/metacritic/MetacriticExample.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6759f61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer les datasets scrapés après découpage\n",
    "df_games_1 = \\\n",
    "pd.read_csv('../data/temp/metacritic/MetacriticPart1.csv',sep=',',index_col='Unnamed: 0')\n",
    "\n",
    "df_games_2 = \\\n",
    "pd.read_csv('../data/temp/metacritic/MetacriticPart2.csv',sep=',',index_col='Unnamed: 0')\n",
    "\n",
    "df_games_3 = \\\n",
    "pd.read_csv('../data/temp/metacritic/MetacriticPart3.csv',sep=',',index_col='Unnamed: 0')\n",
    "\n",
    "df_games_4 = \\\n",
    "pd.read_csv('../data/temp/metacritic/MetacriticPart4.csv',sep=',',index_col='Unnamed: 0')\n",
    "\n",
    "df_games_5 = \\\n",
    "pd.read_csv('../data/temp/metacritic/MetacriticPart5.csv',sep=',',index_col='Unnamed: 0')\n",
    "\n",
    "df_games_6 = \\\n",
    "pd.read_csv('../data/temp/metacritic/MetacriticPart6.csv',sep=',',index_col='Unnamed: 0')\n",
    "\n",
    "\n",
    "df_games_7 = \\\n",
    "pd.read_csv('../data/temp/metacritic/MetacriticPart7.csv',sep=',',index_col='Unnamed: 0')\n",
    "\n",
    "df_games_8 = \\\n",
    "pd.read_csv('../data/temp/metacritic/MetacriticPart8.csv',sep=',',index_col='Unnamed: 0')\n",
    "\n",
    "df_games_9 = \\\n",
    "pd.read_csv('../data/temp/metacritic/MetacriticPart9.csv',sep=',',index_col='Unnamed: 0')\n",
    "\n",
    "df_games_10 = \\\n",
    "pd.read_csv('../data/temp/metacritic/MetacriticPart10.csv',sep=',',index_col='Unnamed: 0')\n",
    "\n",
    "df_games_11 = \\\n",
    "pd.read_csv('../data/temp/metacritic/MetacriticPart11.csv',sep=',',index_col='Unnamed: 0')\n",
    "\n",
    "df_games_12 = \\\n",
    "pd.read_csv('../data/temp/metacritic/MetacriticPart12.csv',sep=',',index_col='Unnamed: 0')\n",
    "\n",
    "df_games_13 = \\\n",
    "pd.read_csv('../data/temp/metacritic/MetacriticPart13.csv',sep=',',index_col='Unnamed: 0')\n",
    "\n",
    "df_games_14 = \\\n",
    "pd.read_csv('../data/temp/metacritic/MetacriticPart14.csv',sep=',',index_col='Unnamed: 0')\n",
    "\n",
    "df_games_15 = \\\n",
    "pd.read_csv('../data/temp/metacritic/MetacriticPart15.csv',sep=',',index_col='Unnamed: 0')\n",
    "\n",
    "df_games_16 = \\\n",
    "pd.read_csv('../data/temp/metacritic/MetacriticPart16.csv',sep=',',index_col='Unnamed: 0')\n",
    "\n",
    "df_games_17 = \\\n",
    "pd.read_csv('../data/temp/metacritic/MetacriticPart17.csv',sep=',',index_col='Unnamed: 0')\n",
    "\n",
    "df_games_18 = \\\n",
    "pd.read_csv('../data/temp/metacritic/MetacriticPart18.csv',sep=',',index_col='Unnamed: 0')\n",
    "\n",
    "df_games_19 = \\\n",
    "pd.read_csv('../data/temp/metacritic/MetacriticPart19.csv',sep=',',index_col='Unnamed: 0')\n",
    "\n",
    "df_games_20 = \\\n",
    "pd.read_csv('../data/temp/metacritic/MetacriticPart20.csv',sep=',',index_col='Unnamed: 0')\n",
    "\n",
    "df_games_21 = \\\n",
    "pd.read_csv('../data/temp/metacritic/MetacriticPart21.csv',sep=',',index_col='Unnamed: 0')\n",
    "#Quelques nan au niveau des plateformes içi\n",
    "\n",
    "df_games_22 = \\\n",
    "pd.read_csv('../data/temp/metacritic/MetacriticPart22.csv',sep=',',index_col='Unnamed: 0')\n",
    "\n",
    "df_games_23 = \\\n",
    "pd.read_csv('../data/temp/metacritic/MetacriticPart23.csv',sep=',',index_col='Unnamed: 0')\n",
    "\n",
    "df_games_24 = \\\n",
    "pd.read_csv('../data/temp/metacritic/MetacriticPart24.csv',sep=',',index_col='Unnamed: 0')\n",
    "\n",
    "\n",
    "df_games = pd.concat([\n",
    "    df_games_1,df_games_2,df_games_3,df_games_4,df_games_5,df_games_6,df_games_7,df_games_8,df_games_9,df_games_10,\n",
    "    df_games_11,df_games_12,df_games_13,df_games_14,df_games_15,df_games_16,df_games_17,df_games_18,df_games_19,\n",
    "    df_games_20,df_games_21,df_games_22,df_games_23,df_games_24\n",
    "])\n",
    "\n",
    "df_games.to_csv('../data/temp/metacritic/AllPartsMetacritic.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c82a9d55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Renommage des colonnes du dataframe \n",
    "df_games = df_games.rename(\n",
    "    columns={\n",
    "        'plateforme-url':'Plateforme',\n",
    "        'jeu':'Name','date':'Date',\n",
    "        'studio':'Studio','metascore':'Metascore',\n",
    "        'userscore':'Userscore'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3b8497fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Supprimer les lignes avec les colonnes suivantes à nan (Jeu,Date, Plateform,studio)\n",
    "\"\"\"\n",
    "# Données non correctes\n",
    "df_games = df_games.dropna(axis = 0, how = 'all', subset = ['Date','Plateforme','Studio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ce59a92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convertir les dates\n",
    "\"\"\"\n",
    "\n",
    "#Supprimer les nan dans la date \n",
    "df_games = df_games.dropna(axis = 0, how = 'all', subset = ['Date'])\n",
    "years = []\n",
    "for d in df_games['Date']:\n",
    "    s = d[-5:]\n",
    "    years.append(s)\n",
    "\n",
    "df_games['Year'] = years\n",
    "\n",
    "# Supprimer les lignes avec les dates incorrectes \n",
    "df_games = df_games[(df_games['Year']!='TBA') & (df_games['Year']!='ccess')]\n",
    "\n",
    "#Mettre la date de sortie au format MM/AAAA\n",
    "def __ManageDate(d):\n",
    "    if 'TBA' in str(d):\n",
    "        d = d.strip('TBA')\n",
    "        date = 'Jan 01,' + str(d)\n",
    "    elif 'October 2009' in d:\n",
    "        d = d.strip('October')\n",
    "        date = 'Oct 01,' + str(d)\n",
    "    elif len(d.strip())==4:\n",
    "        date = 'Jan 01, ' + str(d)\n",
    "    else:\n",
    "        date = d\n",
    "    return datetime.strptime(date, '%b %d, %Y')\n",
    "    \n",
    "df_games['Date'] = df_games['Date'].apply(lambda x : __ManageDate(x))\n",
    "\n",
    "\"\"\"\n",
    "Mettre à jour les types \n",
    "\"\"\"\n",
    "df_games['Year'] = df_games['Year'].astype(int)\n",
    "df_games[['Metascore','Userscore']] = df_games[['Metascore','Userscore']].astype(float)\n",
    "#Mettre les notes Metacritic à l'échelle \n",
    "df_games['Metascore'] = df_games['Metascore'].apply(lambda x : x/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "07673aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supprimer les doublons\n",
    "df_games = df_games.drop_duplicates(keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c60a5269",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Restreindre notre dataset aux jeux et plateformes présentes dans le dataset initial\n",
    "\"\"\"\n",
    "#Plateforme non disponible dans le dataset initial\n",
    "plateformeNotExist = ['PlayStation 5','Xbox Series X','Switch','iOS','Stadia'] \n",
    "\n",
    "#Filtrage des données pour les plateformes existentes dans le dataset initial\n",
    "df_games = df_games[~df_games['Plateforme'].isin(plateformeNotExist)]\n",
    "#96978 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "67db63fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Table de correspondance des plateformes.\n",
    "Cette table sert à joindre les dénomminations des plateformes du dataset Metacritic et du dataset inital\n",
    "\"\"\"\n",
    "cor_Plateform = [\n",
    "    {\n",
    "        'metacritic':'Wii',\n",
    "        'source':'Wii'\n",
    "    },\n",
    "    \n",
    "     {\n",
    "        'metacritic':'Xbox 360',\n",
    "        'source':'X360'\n",
    "    },\n",
    "    {\n",
    "        'metacritic':'PlayStation 3',\n",
    "        'source':'PS3'\n",
    "    },\n",
    "    {\n",
    "        'metacritic':'PlayStation 2',\n",
    "        'source':'PS2'\n",
    "    },\n",
    "    {\n",
    "        'metacritic':'Game Boy Advance',\n",
    "        'source':'GBA'\n",
    "    },\n",
    "    {\n",
    "        'metacritic':'PlayStation 4',\n",
    "        'source':'PS4'\n",
    "    },\n",
    "    {\n",
    "        'metacritic':'Nintendo 64',\n",
    "        'source':'N64'\n",
    "    },\n",
    "    {\n",
    "        'metacritic':'PlayStation',\n",
    "        'source':'PS'\n",
    "    },\n",
    "     {\n",
    "        'metacritic':'PC',\n",
    "        'source':'PC'\n",
    "    },\n",
    "    {\n",
    "        'metacritic':'PSP',\n",
    "        'source':'PSP'\n",
    "    },\n",
    "     {\n",
    "        'metacritic':'Xbox One',\n",
    "        'source':'XOne'\n",
    "    },\n",
    "     {\n",
    "        'metacritic':'GameCube',\n",
    "        'source':'GC'\n",
    "    },\n",
    "     {\n",
    "        'metacritic':'Wii U',\n",
    "        'source':'WiiU'\n",
    "    },\n",
    "    {\n",
    "        'metacritic':'Dreamcast',\n",
    "        'source':'DC'\n",
    "    },\n",
    "     {\n",
    "        'metacritic':'PlayStation Vita',\n",
    "        'source':'PSV'\n",
    "     },\n",
    "    {\n",
    "        'metacritic':'DS',\n",
    "        'source':'DS'\n",
    "     },\n",
    "    {\n",
    "        'metacritic':'3DS',\n",
    "        'source':'3DS'\n",
    "     },\n",
    "    {\n",
    "        'metacritic':'Xbox',\n",
    "        'source':'XB'\n",
    "     }\n",
    "    \n",
    "]\n",
    "\n",
    "df_cor = pd.DataFrame(cor_Plateform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "45e0e699",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Merger le dataset Metacritic avec la correspondances des plateformes\n",
    "\"\"\"\n",
    "df_games = df_games.merge(df_cor,left_on='Plateforme',right_on='metacritic', how='left').drop_duplicates()\n",
    "df_games = df_games.drop(columns=['metacritic'])\n",
    "df_games = df_games.rename(columns={'source':'CodePlateforme'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "686de96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Préparation à la jointure avec les autres datasets\n",
    "\"\"\"\n",
    "\n",
    "newGamesNames = []\n",
    "for d in df_games['Name']:\n",
    "    newname = ''.join(e for e in d if e.isalnum())\n",
    "    newGamesNames.append(newname.lower())\n",
    "\n",
    "df_games['NameFormated'] = newGamesNames\n",
    "df_games['join'] = df_games['CodePlateforme'] + df_games['NameFormated']\n",
    "df_games = df_games.drop(columns=['Plateforme','Name','Year','CodePlateforme','CodePlateforme'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c20f13ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_games.to_csv('../data/01_GameSpy_Scraping_Metacritic.csv',sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
